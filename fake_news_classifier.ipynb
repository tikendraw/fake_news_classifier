{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEzAsBIOMloc"
   },
   "source": [
    "# FAKE NEWS CLASSIFIER : Build a system to identify unreliable news articles\n",
    "Develop a machine learning program to identify when an article might be fake news. Run by the UTK Machine Learning Club.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oODj5SvHMPXG"
   },
   "source": [
    "## Dataset Description\n",
    "**train.csv** : A full training dataset with the following attributes:\n",
    "\n",
    "* id: unique id for a news article\n",
    "* title: the title of a news article\n",
    "* author: author of the news article\n",
    "* text: the text of the article; could be incomplete\n",
    "* label: a label that marks the article as potentially unreliable\n",
    "\n",
    "    1: unreliable\n",
    "    \n",
    "    0: reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIv9bTXG7Ipy",
    "outputId": "28778f16-5193-416f-8c4b-d1380d2d728e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy pandas matplotlib -q\n",
    "# !pip install nltk textblob -q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import textblob\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# !git clone https://github.com/tikendraw/funcyou.git -q\n",
    "import funcyou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "id": "fBUgHWJuK4Wf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9iLPf9nh7U31",
    "outputId": "a1b65269-f7de-4f70-8331-80f6e235e375",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DOWNLOADING EXTRA FILES\n",
    "# nltk.download('all')\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNiI7DQH7eG7"
   },
   "source": [
    "# GET THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "43uazWbA8tjw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from funcyou.dataset import download_kaggle_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_GQR_s48q2h",
    "outputId": "63ebb69b-f95f-4829-b733-87cf54f5c948",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORT THE DATA\n",
    "DATA_LINK = 'https://www.kaggle.com/competitions/fake-news/code'\n",
    "\n",
    "# download_kaggle_dataset(url = DATA_LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oO9XUdXN9oa9",
    "outputId": "f4389757-665a-4301-daac-8b2139797fac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNZIP THE DATA\n",
    "# !unzip fake-news.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCwL2MrT-twt"
   },
   "source": [
    "## READ THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yQOZPJV_GZX",
    "outputId": "3d67e863-e7d8-4d6c-9b05-fbc2112128b3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datset.shape:  (20800, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n",
      "dataset info:  None\n"
     ]
    }
   ],
   "source": [
    "data0 = pd.read_csv('./dataset/train.csv')\n",
    "print('datset.shape: ',data0.shape)\n",
    "print('dataset info: ',data0.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data0.copy()\n",
    "data = data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "KgpINmqF_GWi",
    "outputId": "1441fa5a-65f5-44cb-ca0a-ea171272831d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jackie Mason: Hollywood Would Love Trump if He...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Life: Life Of Luxury: Elton John’s 6 Favorite ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ever wonder how Britain’s most iconic pop pian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Benoît Hamon Wins French Socialist Party’s Pre...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>PARIS  —   France chose an idealistic, traditi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Excerpts From a Draft Script for Donald Trump’...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Donald J. Trump is scheduled to make a highly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A Back-Channel Plan for Ukraine and Russia, Co...</td>\n",
       "      <td>Megan Twohey and Scott Shane</td>\n",
       "      <td>A week before Michael T. Flynn resigned as nat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2   2                  Why the Truth Might Get You Fired   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4   4  Iranian woman jailed for fictional unpublished...   \n",
       "5   5  Jackie Mason: Hollywood Would Love Trump if He...   \n",
       "6   6  Life: Life Of Luxury: Elton John’s 6 Favorite ...   \n",
       "7   7  Benoît Hamon Wins French Socialist Party’s Pre...   \n",
       "8   8  Excerpts From a Draft Script for Donald Trump’...   \n",
       "9   9  A Back-Channel Plan for Ukraine and Russia, Co...   \n",
       "\n",
       "                         author  \\\n",
       "0                 Darrell Lucus   \n",
       "1               Daniel J. Flynn   \n",
       "2            Consortiumnews.com   \n",
       "3               Jessica Purkiss   \n",
       "4                Howard Portnoy   \n",
       "5               Daniel Nussbaum   \n",
       "6                           NaN   \n",
       "7               Alissa J. Rubin   \n",
       "8                           NaN   \n",
       "9  Megan Twohey and Scott Shane   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "5  In these trying times, Jackie Mason is the Voi...      0  \n",
       "6  Ever wonder how Britain’s most iconic pop pian...      1  \n",
       "7  PARIS  —   France chose an idealistic, traditi...      0  \n",
       "8  Donald J. Trump is scheduled to make a highly ...      0  \n",
       "9  A week before Michael T. Flynn resigned as nat...      0  "
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0lIRYLU_GUr"
   },
   "source": [
    "\n",
    "\n",
    "Here We will be dropping `id` (unique value does not provide any value to \n",
    "\n",
    "instance) and `author` (to no create a biasness towards it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "id": "s_hK1MAS_GTQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DROPPING ID AND AUTHOR\n",
    "data.drop(['id','author'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "j56TeGol_GRr",
    "outputId": "2a468c9f-91d2-4027-ad76-bf1d4d31910a",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2                  Why the Truth Might Get You Fired   \n",
       "3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4  Iranian woman jailed for fictional unpublished...   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ95Pq4E_GQA"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dvnHFKuI89V"
   },
   "source": [
    "### Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpdIH1dE_GOc",
    "outputId": "6831b7f0-1753-4a30-f61f-c929604cf714",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK FOR NAN VALUES\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsvnSrDE_GMx",
    "outputId": "eebd9388-ae4d-4357-89e9-97ffdb3eac82",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjndL91D_GJa",
    "outputId": "0c619ba1-2007-4b7f-9557-3f858bf531d7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK IF TEXT AND TITLE IS MISSING TOGATHER\n",
    "data[(data['text'] == np.nan) & (data['title']== np.nan)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOOZSlPIDKL6"
   },
   "source": [
    "There are no values where both 'text' and 'title' are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "id": "t8rYyhHFDg32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_nulls = data[data['text'].isnull()].index.tolist()\n",
    "title_nulls  = data[data['title'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkeN1DV6_GCG",
    "outputId": "caaa613d-d367-434a-9c53-ed80d2243a74",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing title values:  0\n",
      "missing title label counts:  Series([], Name: label, dtype: int64)\n",
      "\n",
      "missing text values:  0\n",
      "missing TEXT  label counts:  Series([], Name: label, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# VALUE COUNTS OF MISSING TEXT AND TITLE\n",
    "print('missing title values: ', len(title_nulls))\n",
    "print('missing title label counts: ', data[data['title'].isnull()]['label'].value_counts())\n",
    "\n",
    "# VALUE COUNTS OF MISSING TITLE AND TITLE\n",
    "print('\\nmissing text values: ', len(text_nulls))\n",
    "print('missing TEXT  label counts: ', data[data['text'].isnull()]['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwAQ3rBu_GBC"
   },
   "source": [
    "Here we can see that if either text or title is missing then it is classified as Fake news explicitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5EG8X5G_F_L"
   },
   "source": [
    "So, we dropping nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIsnv1Va_F9O",
    "outputId": "59f6daf8-e31e-42c0-b33a-fbe9e3671084",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropping na:  (10, 3)\n",
      "after dropping na:  (10, 3)\n"
     ]
    }
   ],
   "source": [
    "print('before dropping na: ',data.shape)\n",
    "data.dropna(axis = 0, inplace = True)\n",
    "print('after dropping na: ',data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojR1GvPB_FtP"
   },
   "source": [
    "## Replacing Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "cellView": "form",
    "id": "lAZExuJj_Fr9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title this is contractions :: too long don't open\n",
    "contractions_dict = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "id": "hm70b9MM_FlV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS TO EXPAND CONTRACTIONS\n",
    "def cont_to_exp(x):\n",
    "    x = str(x).lower()\n",
    "    xsplited = x.split(' ')\n",
    "    exp_sentence = []\n",
    "    for s in x.split():\n",
    "        if s in contractions_dict.keys():\n",
    "            \n",
    "            s = contractions_dict.get(s)\n",
    "        exp_sentence.append(s)\n",
    "        \n",
    "    x = ' '.join(exp_sentence)\n",
    "    return x\n",
    "#     print(xsplited)\n",
    "#     return xsplited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It'"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ss = \"you Didn't he don't they can't\"\n",
    "ss  = data.title[0]\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'did not'"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions_dict.get(\"didn't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DSpdf_q9_Fi9",
    "outputId": "a177d411-49ec-4557-8e73-80fab3fcafbf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'dem', 'aide:', 'we', 'didn’t', 'even', 'see', 'comey’s', 'letter', 'until', 'jason', 'chaffetz', 'tweeted', 'it']\n"
     ]
    }
   ],
   "source": [
    "sss = cont_to_exp(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss[4] in contractions_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nMow2Yn_FiA",
    "outputId": "7f31c99a-791f-4396-c29a-98371e10175e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.72 ms, sys: 0 ns, total: 6.72 ms\n",
      "Wall time: 7.43 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text'] = data['text'].apply(cont_to_exp)\n",
    "data['title'] = data['title'].apply(cont_to_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house dem aide: we didn’t even see comey’s let...</td>\n",
       "      <td>house dem aide: we didn’t even see comey’s let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flynn: hillary clinton, big woman on campus - ...</td>\n",
       "      <td>ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the truth might get you fired</td>\n",
       "      <td>why the truth might get you fired october 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 civilians killed in single us airstrike hav...</td>\n",
       "      <td>videos 15 civilians killed in single us airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>print an iranian woman has been sentenced to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  house dem aide: we didn’t even see comey’s let...   \n",
       "1  flynn: hillary clinton, big woman on campus - ...   \n",
       "2                  why the truth might get you fired   \n",
       "3  15 civilians killed in single us airstrike hav...   \n",
       "4  iranian woman jailed for fictional unpublished...   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide: we didn’t even see comey’s let...      1  \n",
       "1  ever get the feeling your life circles the rou...      0  \n",
       "2  why the truth might get you fired october 29, ...      1  \n",
       "3  videos 15 civilians killed in single us airstr...      1  \n",
       "4  print an iranian woman has been sentenced to s...      1  "
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # removing punctuation\n",
    "    # remove special characters from text column\n",
    "    text = re.sub('[#,@,&]', '',text)\n",
    "    # Remove digits\n",
    "    text = re.sub('\\d*','', text)\n",
    "    # remove \"'s\"\n",
    "    text = re.sub(\"'s\",'', text)\n",
    "    #Remove www\n",
    "    text = re.sub('w{3}','', text)\n",
    "    # remove urls\n",
    "    text = re.sub(\"http\\S+\", \"\", text)\n",
    "    # remove multiple spaces with single space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 ms, sys: 0 ns, total: 27.6 ms\n",
      "Wall time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['title'] = data['title'].apply(text_cleaning) \n",
    "data['text'] = data['text'].apply(text_cleaning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house dem aide we didn even see comey letter u...</td>\n",
       "      <td>house dem aide we didn even see comey letter u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flynn hillary clinton big woman on campus brei...</td>\n",
       "      <td>ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the truth might get you fired</td>\n",
       "      <td>why the truth might get you fired october the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>civilians killed in single us airstrike have ...</td>\n",
       "      <td>videos civilians killed in single us airstrike...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>print an iranian woman has been sentenced to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  house dem aide we didn even see comey letter u...   \n",
       "1  flynn hillary clinton big woman on campus brei...   \n",
       "2                  why the truth might get you fired   \n",
       "3   civilians killed in single us airstrike have ...   \n",
       "4  iranian woman jailed for fictional unpublished...   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide we didn even see comey letter u...      1  \n",
       "1  ever get the feeling your life circles the rou...      0  \n",
       "2  why the truth might get you fired october the ...      1  \n",
       "3  videos civilians killed in single us airstrike...      1  \n",
       "4  print an iranian woman has been sentenced to s...      1  "
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 525 ms, sys: 52.3 ms, total: 578 ms\n",
      "Wall time: 576 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# takes alot of time\n",
    "data['title'] = data['title'].apply(nltk_clean) \n",
    "data['text'] = data['text'].apply(nltk_clean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>house dem aide even see comey letter jason cha...</td>\n",
       "      <td>house dem aide even see comey letter jason cha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flynn hillary clinton big woman campus breitbart</td>\n",
       "      <td>ever get feeling life circle roundabout rather...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>truth might get fired</td>\n",
       "      <td>truth might get fired october tension intellig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>civilian killed single u airstrike identified</td>\n",
       "      <td>video civilian killed single u airstrike ident...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iranian woman jailed fictional unpublished sto...</td>\n",
       "      <td>print iranian woman sentenced six year prison ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  house dem aide even see comey letter jason cha...   \n",
       "1   flynn hillary clinton big woman campus breitbart   \n",
       "2                              truth might get fired   \n",
       "3      civilian killed single u airstrike identified   \n",
       "4  iranian woman jailed fictional unpublished sto...   \n",
       "\n",
       "                                                text  label  \n",
       "0  house dem aide even see comey letter jason cha...      1  \n",
       "1  ever get feeling life circle roundabout rather...      0  \n",
       "2  truth might get fired october tension intellig...      1  \n",
       "3  video civilian killed single u airstrike ident...      1  \n",
       "4  print iranian woman sentenced six year prison ...      1  "
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "TtHCVDcH_FgD",
    "outputId": "741c93e0-b13d-492f-f850-0bbd41c17589",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW1tA6rB_Fep"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyWuKE9w_Fco",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['title_len'] = data['title'].apply(lambda x: len(str(x).split()))\n",
    "data['text_len'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "data['title_text_ratio'] = data['title_len']/data['text_len']\n",
    "# train['avg_title_len'] = train['title'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "BRr5g2HY_FZT",
    "outputId": "57a71aa9-0224-4085-e00f-9fb2c2421875",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "VHi8kbvwbDrM",
    "outputId": "a6d068b7-7f8b-4f8d-b21c-b8ab0394e92c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Title Word Count distribution\n",
    "plt.figure(figsize = (15,7))\n",
    "plt.grid()\n",
    "plt.hist(x = data.title_len, bins=25)\n",
    "plt.xlabel('word length')\n",
    "plt.ylabel('Count')\n",
    "# plt.plot(x = , y = 0)\n",
    "percent = 99\n",
    "for i in range(95,100):\n",
    "    plt.axvline(x = np.percentile(data.title_len, i), color = 'b', label = 'axvline - full height')\n",
    "plt.title(f'Title Word Count distribution: {np.percentile(data.title_len, percent)} words cover {percent}% of title data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "S0idro9ifJZ5",
    "outputId": "16c1b115-4198-4ce8-9d6a-c73565083ae4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Title Word Count distribution\n",
    "plt.figure(figsize = (15,7))\n",
    "plt.grid()\n",
    "plt.hist(x = data.text_len, bins=25)\n",
    "plt.xlabel('word length')\n",
    "plt.ylabel('Count')\n",
    "# plt.plot(x = , y = 0)\n",
    "percent = 99\n",
    "for i in range(95,100):\n",
    "    plt.axvline(x = np.percentile(data.text_len, i), color = 'b', label = 'axvline - full height')\n",
    "plt.title(f'Text Word Count distribution: {np.percentile(data.text_len, percent)} words cover {percent}% of text data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z147GJikd5mN"
   },
   "source": [
    "so word lenghth upto 22 covers 99% of the dataset title\n",
    "so word lenghth upto 4061 covers 99% of the dataset text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMzg6P0U_FW-"
   },
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8FYNVAP_FUl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "UyVdezFA5CJg",
    "outputId": "05b570ba-dfae-4e04-f060-3b18edb46bec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = data.text[11]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Quf6G8vV_FOz",
    "outputId": "7bd642c9-1ad5-47e4-cc79-e724238c943a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "tfidf = TfidfVectorizer()\n",
    "ddd = tfidf.fit_transform(np.array([s]))\n",
    "ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUxY_1R2_FBH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "y0qR_1mD8b_p",
    "outputId": "bae21f8d-befe-4512-bea9-faf298460e69",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[['text','title','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rg4Vd1tATFP"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFMFybLvASGp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import datetime\n",
    "\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "  Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zcf_IY5nUxmy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    \"\"\"\n",
    "    Creates a TensorBoard callback instand to store log files.\n",
    "    Stores log files with the filepath:\n",
    "    \"dir_name/experiment_name/current_datetime/\"\n",
    "    Args:\n",
    "    dir_name: target directory to store TensorBoard log files\n",
    "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
    "    \"\"\"\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir\n",
    "    )\n",
    "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "    return tensorboard_callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYCxuEbx8GpN"
   },
   "source": [
    "# Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwAYgl0Y8RT5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(data[['title', 'text']], data['label'], test_size = .1, random_state=3, stratify = data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVS6xmT69LtI",
    "outputId": "1b0d87c8-87ef-4b69-d666-4f5fe98fe571",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('xtrain: ', xtrain.shape)\n",
    "print('ytrain: ', ytrain.shape)\n",
    "print('xtest: ', xtest.shape)\n",
    "print('ytest: ', ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR65StrW_E-b"
   },
   "source": [
    "# Baseline : Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yl8DR4a-_E79",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxB-WRIf_E5S",
    "outputId": "59f8dd8d-0cdd-48a1-f3d5-0c5e2a0f7fdd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model0 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "#fit model\n",
    "model0.fit(xtrain.title.to_list(), ytrain.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYv_3EjD_E18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ytrue0 = ytest.to_list()\n",
    "ypred0 = model0.predict(xtest.title.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtm5_5lG_EzO",
    "outputId": "c0d8a134-66ec-44f3-d14d-add6bfdecbc8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_results(ytrue0, ypred0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhPJC09lURkP"
   },
   "source": [
    "# Token Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWbytWt0VTIS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G6ZxzPqWHy6",
    "outputId": "cb42b254-60d1-4a3f-e6ea-a75a059b1a97",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of unique words in dataset\n",
    "# %%time\n",
    "all_title_words_list = [words.split() for words in xtrain.title]\n",
    "all_title_words = set(num for sublist in all_title_words_list for num in sublist)\n",
    "print('total token in titles: ',len(all_title_words))\n",
    "\n",
    "all_text_words_list = [words.split() for words in xtrain.text]\n",
    "all_text_words = set(num for sublist in all_text_words_list for num in sublist)\n",
    "print('total token in text: ',len(all_text_words))\n",
    "\n",
    "all_words_combined_list = all_text_words_list + all_title_words_list\n",
    "all_words_combined = set(num for sublist in all_words_combined_list for num in sublist)\n",
    "\n",
    "print('total token combined: ',len(all_words_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJVYZJ8razeb",
    "outputId": "0c94aed0-bfa8-4753-8167-83cbccf618be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output_sequence_length\n",
    "percent_of_the_data_to_cover = 99\n",
    "output_sequence_len_title = int(np.percentile(data.title_len, percent_of_the_data_to_cover))\n",
    "print('output_sequence_len_title: ',output_sequence_len_title)\n",
    "output_sequence_len_text = int(np.percentile(data.text_len, percent_of_the_data_to_cover))\n",
    "print('output_sequence_len_text: ',output_sequence_len_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpORWcfEmzYR"
   },
   "source": [
    "we do not want every word to tokenize , there happens to be alot of less occuring words that we do not want. Thats why we will substract 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZClqL1gWBED",
    "outputId": "82d94571-2897-40cc-95a5-3f2e1c9c31e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_token_title = len(all_title_words) - 2500          #number of words to tokenize -500 as we do not want every word to tokenize \n",
    "print(max_token_title)\n",
    "max_token_text = len(all_text_words) -100_000\n",
    "print(max_token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G77Vrdd7Vr9J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VECTORIZER FOR TITLE\n",
    "title_text_vectorizer = TextVectorization(max_tokens=max_token_title, \n",
    "                                          output_sequence_length=output_sequence_len_title,\n",
    "                                          pad_to_max_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mpKov53iUEk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VECTORIZER FOR TEXT\n",
    "text_text_vectorizer = TextVectorization(max_tokens=max_token_text, \n",
    "                                          output_sequence_length=output_sequence_len_text,\n",
    "                                          pad_to_max_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qe86NXv5in16",
    "outputId": "456b0970-b775-4962-9c15-6094582d8ee0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapt text vectorizer to training titles\n",
    "# %%time\n",
    "title_text_vectorizer.adapt(xtrain.title.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxpBoQ4Kld_g",
    "outputId": "41219adc-aee2-4a83-ba54-7c1407084d85",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many words are there\n",
    "total_words =  len(title_text_vectorizer.get_vocabulary())\n",
    "print('total no. of words: ', total_words)\n",
    "print('5 Most frequent words', title_text_vectorizer.get_vocabulary()[:5])\n",
    "print('5 Least frequent words', title_text_vectorizer.get_vocabulary()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ir9sHImXkgx9",
    "outputId": "04ff13e6-cab5-44cb-8e8e-288f95eb80b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test out text vectorizer\n",
    "import random\n",
    "target_title_sentence = random.choice(xtest.title.to_list())\n",
    "print(f\"Text:\\n{target_title_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_title_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{title_text_vectorizer([target_title_sentence])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq07Ix54XQX0"
   },
   "source": [
    "**Note:** Here instead of Directly adapting to a list of string, we will convert it into \n",
    "tf.data.dataset to overcome fitting it the memory problem. It crashed the system if used a list. while a tf.data adjust according to the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsLLNcSnjMKi",
    "outputId": "2aed2898-84ae-48be-e902-f1e094995ace",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapt text vectorizer to training text\n",
    "# %%time\n",
    "\n",
    "train_text = tf.data.Dataset.from_tensor_slices(xtrain.text.to_list())\n",
    "text_text_vectorizer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmobBWb7ln5v",
    "outputId": "829812d8-501a-4c18-c515-d262b33edd77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many words are there\n",
    "total_words =  len(text_text_vectorizer.get_vocabulary())\n",
    "print('total no. of words: ', total_words)\n",
    "print('5 Most frequent words', text_text_vectorizer.get_vocabulary()[:5])\n",
    "print('5 Least frequent words', text_text_vectorizer.get_vocabulary()[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv0mNOldl1Gv"
   },
   "source": [
    "As we have seen above there are low frequency words that do not need to tokenize\n",
    "as they occure less. so set max_token to less than actual unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQFaJIpnjMJF",
    "outputId": "42f3063d-f83a-4dce-d176-04439986e2d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test out text vectorizer\n",
    "import random\n",
    "target_text_sentence = random.choice(xtest.text.to_list())\n",
    "print(f\"Text:\\n{target_text_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_text_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_text_vectorizer([target_text_sentence])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvV_hn97GmqD"
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nOKht4HGmnO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDyU42mtJwDz"
   },
   "source": [
    "### Title Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nHO5tlUIq1z",
    "outputId": "a54a05ad-c24a-43ec-95ac-b46a557a3316",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_title_words =  len(title_text_vectorizer.get_vocabulary())\n",
    "print(total_title_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7XjSQ8eGmlZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_embedder = Embedding(input_dim = total_title_words,\n",
    "                           output_dim = 32,\n",
    "                           mask_zero = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMvZxku8Gmhl",
    "outputId": "15422c91-411e-4eef-9758-972e6157af8a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('sentence: ', target_title_sentence ,end = '\\n')\n",
    "print('\\n')\n",
    "vectorized_sentence = title_text_vectorizer([target_title_sentence])\n",
    "print('vectorized: ', vectorized_sentence)\n",
    "print('\\n')\n",
    "embedded_sentence = title_embedder(vectorized_sentence)\n",
    "print('embedded shape', embedded_sentence.shape)\n",
    "print('embedded: ', embedded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RbfOA38JrKh"
   },
   "source": [
    "### Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AknweDF6GmgO",
    "outputId": "94057179-721a-4769-fc05-f14a72f2c8ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_text_words =  len(text_text_vectorizer.get_vocabulary())\n",
    "print(total_text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8UT_Yy_Gmcy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_embedder = Embedding(input_dim = total_text_words,\n",
    "                           output_dim = 32,\n",
    "                           mask_zero = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_V_OraOkGmY7",
    "outputId": "6b75fe3b-be6c-402e-d94a-67ecc0c6a1af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('sentence: ', target_text_sentence ,end = '\\n')\n",
    "print('sentence len: ', len(target_text_sentence) ,end = '\\n')\n",
    "\n",
    "print('\\n')\n",
    "vectorized_sentence = title_text_vectorizer([target_text_sentence])\n",
    "print('vectorized: ', vectorized_sentence)\n",
    "print('\\n')\n",
    "embedded_sentence = text_embedder(vectorized_sentence)\n",
    "print('embedded shape', embedded_sentence.shape)\n",
    "print('embedded: ', embedded_sentence[:,:5,:5]) #This is too long to print \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDFXJJlqOjSa"
   },
   "source": [
    "# Creating `tf.Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "377zdUTkO20W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train tf.data\n",
    "# train_title = tf.data.Dataset.from_tensor_slices(xtrain.title.to_list())\n",
    "# train_text = tf.data.Dataset.from_tensor_slices(xtrain.text.to_list())\n",
    "train_label = tf.data.Dataset.from_tensor_slices(ytrain)\n",
    "\n",
    "\n",
    "train_features = tf.data.Dataset.from_tensor_slices((xtrain.title.to_list(), xtrain.text.to_list()))\n",
    "train_dataset = tf.data.Dataset.zip((train_features, train_label))\n",
    "\n",
    "# test tf.data\n",
    "# test_title = tf.data.Dataset.from_tensor_slices(xtest.title.to_list())\n",
    "# test_text = tf.data.Dataset.from_tensor_slices(xtest.text.to_list())\n",
    "test_label = tf.data.Dataset.from_tensor_slices(ytest)\n",
    "\n",
    "test_features = tf.data.Dataset.from_tensor_slices((xtest.title.to_list(), xtest.text.to_list()))\n",
    "test_dataset  = tf.data.Dataset.zip((test_features, test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modela = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "#fit model\n",
    "modela.fit(xtrain.title.to_list(), ytrain)\n",
    "# model0.fit(sss, ytrain.to_list())\n",
    "ytruea = ytest.to_list()\n",
    "ypreda = model0.predict(xtest.title.to_list())\n",
    "\n",
    "calculate_results(ytruea, ypreda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZ2G11cGSmbG",
    "outputId": "e4c7fcbb-f044-48ae-c28b-2271d9e9cd16",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "for i,j in train_dataset.take(1):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGzTCzESO2tv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prefetching \n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize shape:  (None, 22)\n",
      "embedded shape:  (None, 22, 32)\n"
     ]
    }
   ],
   "source": [
    "input1 = keras.Input(shape = (1), dtype = tf.string)\n",
    "tokenize = title_text_vectorizer(input1)\n",
    "print('tokenize shape: ', tokenize.shape)\n",
    "\n",
    "embedd = title_embedder(tokenize)\n",
    "print('embedded shape: ',embedd.shape)\n",
    "\n",
    "# a conv1d layer\n",
    "x = layers.Conv1D(32, 5, 1, activation='relu')(embedd)\n",
    "x = layers.Dropout(.3)(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output1 = layers.Dense(1, activation='relu')(x)\n",
    "\n",
    "#compile\n",
    "model1.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tb/model1/20221109-195505\n",
      "Epoch 1/3\n",
      "9/9 [==============================] - 60s 5s/step - loss: nan - accuracy: 0.5347 - val_loss: nan - val_accuracy: 0.5156\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 14s 296ms/step - loss: nan - accuracy: 0.5104 - val_loss: nan - val_accuracy: 0.5156\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 3s 292ms/step - loss: nan - accuracy: 0.5174 - val_loss: nan - val_accuracy: 0.5156\n"
     ]
    }
   ],
   "source": [
    "hist1 = model1.fit(train_dataset, epochs = EPOCHS,\n",
    "                      steps_per_epoch = int(.1* (len(train_dataset)/EPOCHS)),\n",
    "                      validation_steps = int(.2* (len(test_dataset)/EPOCHS)),\n",
    "                      validation_data=test_dataset,\n",
    "                      callbacks = [create_tensorboard_callback('tb','model1')]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT_p6WhpjMGF"
   },
   "source": [
    "# Modelaa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "o9AYnIxajMEp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9cuQS59jMCH",
    "outputId": "7ddb61af-d381-4133-c699-9c3731cc7259",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title output shape:  (None, 32)\n",
      "text output shape:  (None, 32)\n",
      "concatenated shape:  (None, 64)\n"
     ]
    }
   ],
   "source": [
    "input1 = layers.Input(shape = (1,), dtype = tf.string)\n",
    "token = title_text_vectorizer(input1)\n",
    "embed = title_embedder(token)\n",
    "x = layers.LSTM(32,return_sequences=True)(embed)\n",
    "x = layers.LSTM(32,return_sequences=True)(x)\n",
    "title_out = GlobalAveragePooling1D()(x)\n",
    "print('title output shape: ',title_out.shape)\n",
    "\n",
    "input2 = layers.Input(shape = (1,), dtype= tf.string)\n",
    "token2 = text_text_vectorizer(input2)\n",
    "embed2 = text_embedder(token2)\n",
    "x = layers.LSTM(32,return_sequences=True)(embed2)\n",
    "x = layers.LSTM(32,return_sequences=True)(x)\n",
    "text_out = GlobalAveragePooling1D()(x)\n",
    "print('text output shape: ',text_out.shape)\n",
    "\n",
    "concat = layers.Concatenate()([title_out, text_out])\n",
    "print('concatenated shape: ',concat.shape)\n",
    "\n",
    "x = layers.Dropout(.3)(concat)\n",
    "x = layers.Dense(128, activation = 'relu')(x)\n",
    "x = layers.Dense(128, activation = 'relu')(x)\n",
    "outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "modelaa = keras.Model(inputs = [input1,input2], outputs = outputs)\n",
    "\n",
    "#COMPILE\n",
    "modelaa.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "               optimizer = keras.optimizers.Adam(),\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYkA2QVoTQzM",
    "outputId": "921fb00d-5d89-4388-eadc-accc73b9f0bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "_xs5tlRwjL_Z",
    "outputId": "3a012037-ecb7-48de-cdb6-8052a8259008",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "historyaa = modelaa.fit(train_dataset, epochs = EPOCHS,\n",
    "                      steps_per_epoch = int(.5* (len(train_dataset)/EPOCHS)),\n",
    "                      validation_steps = int(.2* (len(test_dataset)/EPOCHS)),\n",
    "                      validation_data=test_dataset,\n",
    "                      callbacks = [create_tensorboard_callback('tb','model1')]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "ybkTe7zKjL9U",
    "outputId": "b80927f0-6d7c-4694-c5d6-2a3620ef17ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "\n",
    "accuracy = history1.history['accuracy']\n",
    "val_accuracy = history1.history['val_accuracy']\n",
    "\n",
    "# print(val_accuracy)\n",
    "plt.figure(figsize = (25,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.grid(True)\n",
    "plt.plot(np.arange(len(loss)), loss, 'r', label='Training loss')\n",
    "plt.plot(np.arange(len(val_loss)), val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.grid(True)\n",
    "plt.plot(np.arange(len(accuracy)), accuracy, 'r', label='Training accuracy')\n",
    "plt.plot(np.arange(len(val_accuracy)), val_accuracy, 'bo', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accracy Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11CtKjszjL7z",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWPQ_JNXjL56",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHNII4XQjL4P",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkryvfmjjL2k",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swPTJ1UajL0M",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPE_ba6ejLwu",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhbxtNP7jLru",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBzKuHIYjLo8",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQiQQagQjLmf",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoX1DcxBjLku",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPTPwYbBjLdU",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A9TzpdfjLbn",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPs9zFI8jLaS",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb5xPJncjLYU",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrN83y3mjLU0",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3S4B-htDjLSV",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ko2VnLZfjLQq",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g50oNoMDjLON",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c9rILbZjLLL",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6Mcfxa1jLIC",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVEEYB_ojLFa",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhAO7pJ6jLCy",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxzqPgIwjLAA",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7r0f3aeejK9A",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS8DZMHJjK6c",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTXVCRjZjK33",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
